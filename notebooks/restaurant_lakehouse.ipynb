{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2796ab29",
   "metadata": {},
   "source": [
    "# DS-2002 Final Capstone Project \n",
    "Restaurant Date Lakehouse (Local Implementation)\n",
    "\n",
    "## Project Overview\n",
    "For this project I did a complete data lakehouse structure designed to be like a single restaurants POS system (inspo from Square as i have had to work with the POS system a lot for work). Goal here is to analyze historical and fake real-time transactions in order to understand what was sold and when by whom. \n",
    "\n",
    "Used Apache Spark, Delta Lake, and Structured Streaming following the bronze to silver to gold design patter from class labs. \n",
    "\n",
    "## Arc Summary:\n",
    "\n",
    "Data sources (3 req)\n",
    "- MySQL (Relational/SQL)\n",
    "    - Customers\n",
    "    - Date Dimension\n",
    "- MongoDM Atlas (NoSQL)\n",
    "    - Menu Items \n",
    "- Local CSV Files (File based source)\n",
    "    - Employees\n",
    "    - Order Types\n",
    "- Streaming JSON FIles (local file stream)\n",
    "    - order events nested line items - this simulating real time POS transactions\n",
    "\n",
    "\n",
    "## Lakehouse Designs\n",
    "- Bronze Layer\n",
    "    - stored as delta tables\n",
    "    - streaming JSON  using spark\n",
    "- Silver Layer (Fact table)\n",
    "    - order line fact records\n",
    "    - integrated w all dim tables\n",
    "- Gold Layer\n",
    "    - aggregated queries that support the analysis\n",
    "\n",
    "## Date Dim \n",
    "- This was created using the SQL script from class and populated into MySQL. \n",
    "\n",
    "## Other\n",
    "- this was deployed entirely locally\n",
    "- AZURE WAS NOT USED - as notified was allowed from TA\n",
    "- spark runs locally using delta lake\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dfeb40",
   "metadata": {},
   "source": [
    "# Env set up and directory config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "074d3a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_DIR: C:\\Users\\evasb_eqbyhhj\\Documents\\DS-2002-Capstone\\notebooks\\data\n",
      "WAREHOUSE_DIR: C:\\Users\\evasb_eqbyhhj\\Documents\\DS-2002-Capstone\\notebooks\\warehouse\n",
      "HADOOP_HOME: C:\\hadoop-3.3.6\n",
      "Python: c:\\Users\\evasb_eqbyhhj\\anaconda3\\envs\\ds2002spark\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import json\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path(\".\").resolve()\n",
    "WAREHOUSE_DIR = PROJECT_ROOT / \"warehouse\"\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "\n",
    "WAREHOUSE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"DATA_DIR:\", DATA_DIR)\n",
    "print(\"WAREHOUSE_DIR:\", WAREHOUSE_DIR)\n",
    "\n",
    "\n",
    "os.environ[\"HADOOP_HOME\"] = r\"C:\\hadoop-3.3.6\"\n",
    "os.environ[\"PATH\"] = r\"C:\\hadoop-3.3.6\\bin\" + os.pathsep + os.environ[\"PATH\"]\n",
    "\n",
    "\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable\n",
    "\n",
    "print(\"HADOOP_HOME:\", os.environ[\"HADOOP_HOME\"])\n",
    "print(\"Python:\", sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "461f67f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MongoDB connection successful\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "import certifi\n",
    "\n",
    "MONGO_URI = (\n",
    "    \"mongodb+srv://sae9fp_db_user:ROM8h9wZs6IKRufd\"\n",
    "    \"@restaurant-cluster.p6k0yes.mongodb.net/\"\n",
    "    \"?appName=restaurant-cluster\"\n",
    ")\n",
    "\n",
    "client = pymongo.MongoClient(MONGO_URI, tlsCAFile=certifi.where())\n",
    "\n",
    "# test connection\n",
    "client.admin.command(\"ping\")\n",
    "print(\"MongoDB connection successful\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0efb092",
   "metadata": {},
   "source": [
    "Spark session initialization w delta lake support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14f3df04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from delta import configure_spark_with_delta_pip\n",
    "\n",
    "builder = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"restaurant-lakehouse\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.pyspark.python\", sys.executable)\n",
    "    .config(\"spark.pyspark.driver.python\", sys.executable)\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    ")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91c8bb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mysql_args = {\n",
    "    \"uid\": \"root\",            \n",
    "    \"pwd\": \"760349Eb!toogie\",  \n",
    "    \"hostname\": \"localhost\",\n",
    "    \"port\": 3306,\n",
    "    \"dbname\": \"restaurant_oltp\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74b4013",
   "metadata": {},
   "source": [
    "helper function for rel data extraction (mysql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e502de03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_sql_dataframe(sql_query: str, **args) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extracts data from a MySQL database using SQLAlchemy\n",
    "    and returns a Pandas DataFrame.\n",
    "    \"\"\"\n",
    "    conn_str = (\n",
    "        f\"mysql+pymysql://{args['uid']}:{args['pwd']}@\"\n",
    "        f\"{args['hostname']}:{args['port']}/{args['dbname']}\"\n",
    "    )\n",
    "    engine = create_engine(conn_str, pool_recycle=3600)\n",
    "\n",
    "    with engine.connect() as conn:\n",
    "        return pd.read_sql(text(sql_query), conn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07947cdd",
   "metadata": {},
   "source": [
    "employee dim - csv source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ec34cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- employee_code: string (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- role: string (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- created_at: timestamp (nullable = false)\n",
      "\n",
      "+-----------+-------------+----------+---------+---------+----------+------+--------------------------+\n",
      "|employee_id|employee_code|first_name|last_name|role     |hire_date |status|created_at                |\n",
      "+-----------+-------------+----------+---------+---------+----------+------+--------------------------+\n",
      "|1          |EMP-001      |Mia       |Lopez    |Server   |2022-05-10|Active|2025-12-14 09:53:36.228236|\n",
      "|2          |EMP-002      |Ethan     |Clark    |Server   |2023-02-18|Active|2025-12-14 09:53:36.228236|\n",
      "|3          |EMP-003      |Sofia     |Nguyen   |Bartender|2021-09-01|Active|2025-12-14 09:53:36.228236|\n",
      "|4          |EMP-004      |Jack      |Reed     |Manager  |2020-03-15|Active|2025-12-14 09:53:36.228236|\n",
      "|5          |EMP-005      |Nora      |Patel    |Host     |2024-08-20|Active|2025-12-14 09:53:36.228236|\n",
      "+-----------+-------------+----------+---------+---------+----------+------+--------------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "employees_path = DATA_DIR / \"employees.csv\"\n",
    "assert employees_path.exists(), f\"Missing {employees_path}\"\n",
    "\n",
    "employees_sdf = (\n",
    "    spark.read\n",
    "         .option(\"header\", \"true\")\n",
    "         .csv(str(employees_path))\n",
    "         .withColumn(\"employee_id\", F.col(\"employee_id\").cast(\"int\"))\n",
    "         .withColumn(\"hire_date\", F.to_date(\"hire_date\"))\n",
    "         .withColumn(\"created_at\", F.current_timestamp())\n",
    ")\n",
    "\n",
    "dim_employee_path = (WAREHOUSE_DIR / \"dim_employee\").resolve().as_posix()\n",
    "\n",
    "(employees_sdf\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .save(dim_employee_path)\n",
    ")\n",
    "\n",
    "employees_sdf.printSchema()\n",
    "employees_sdf.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d7ca34",
   "metadata": {},
   "source": [
    "order_type dim - csv source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf8a71c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_type_id: integer (nullable = true)\n",
      " |-- order_type_key: string (nullable = true)\n",
      " |-- order_type_name: string (nullable = true)\n",
      " |-- channel: string (nullable = true)\n",
      "\n",
      "+-------------+--------------+---------------+-----------+\n",
      "|order_type_id|order_type_key|order_type_name|channel    |\n",
      "+-------------+--------------+---------------+-----------+\n",
      "|1            |DINE_IN       |Dine-in        |On-premise |\n",
      "|2            |TAKEOUT       |Takeout        |Off-premise|\n",
      "|3            |DELIVERY      |Delivery       |Off-premise|\n",
      "+-------------+--------------+---------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "order_types_path = DATA_DIR / \"order_types.csv\"\n",
    "assert order_types_path.exists(), f\"Missing {order_types_path}\"\n",
    "\n",
    "order_types_sdf = (\n",
    "    spark.read\n",
    "         .option(\"header\", \"true\")\n",
    "         .csv(str(order_types_path))\n",
    "         .withColumn(\"order_type_id\", F.col(\"order_type_id\").cast(\"int\"))\n",
    ")\n",
    "\n",
    "dim_order_type_path = (WAREHOUSE_DIR / \"dim_order_type\").resolve().as_posix()\n",
    "\n",
    "(order_types_sdf\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .save(dim_order_type_path)\n",
    ")\n",
    "\n",
    "order_types_sdf.printSchema()\n",
    "order_types_sdf.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a9e85bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>customer_code</th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>email</th>\n",
       "      <th>phone</th>\n",
       "      <th>loyalty_tier</th>\n",
       "      <th>signup_date</th>\n",
       "      <th>birthdate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>CUST-001</td>\n",
       "      <td>Emma</td>\n",
       "      <td>Johnson</td>\n",
       "      <td>emma.j@example.com</td>\n",
       "      <td>555-1111</td>\n",
       "      <td>Gold</td>\n",
       "      <td>2023-01-10</td>\n",
       "      <td>1998-04-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>CUST-002</td>\n",
       "      <td>Liam</td>\n",
       "      <td>Smith</td>\n",
       "      <td>liam.s@example.com</td>\n",
       "      <td>555-2222</td>\n",
       "      <td>Silver</td>\n",
       "      <td>2023-03-15</td>\n",
       "      <td>1995-09-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>CUST-003</td>\n",
       "      <td>Olivia</td>\n",
       "      <td>Brown</td>\n",
       "      <td>olivia.b@example.com</td>\n",
       "      <td>555-3333</td>\n",
       "      <td>Bronze</td>\n",
       "      <td>2024-01-02</td>\n",
       "      <td>2000-07-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>CUST-004</td>\n",
       "      <td>Noah</td>\n",
       "      <td>Davis</td>\n",
       "      <td>noah.d@example.com</td>\n",
       "      <td>555-4444</td>\n",
       "      <td>Gold</td>\n",
       "      <td>2022-11-08</td>\n",
       "      <td>1992-02-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>CUST-005</td>\n",
       "      <td>Ava</td>\n",
       "      <td>Wilson</td>\n",
       "      <td>ava.w@example.com</td>\n",
       "      <td>555-5555</td>\n",
       "      <td>Silver</td>\n",
       "      <td>2023-06-21</td>\n",
       "      <td>1999-11-30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id customer_code first_name last_name                 email  \\\n",
       "0            1      CUST-001       Emma   Johnson    emma.j@example.com   \n",
       "1            2      CUST-002       Liam     Smith    liam.s@example.com   \n",
       "2            3      CUST-003     Olivia     Brown  olivia.b@example.com   \n",
       "3            4      CUST-004       Noah     Davis    noah.d@example.com   \n",
       "4            5      CUST-005        Ava    Wilson     ava.w@example.com   \n",
       "\n",
       "      phone loyalty_tier signup_date   birthdate  \n",
       "0  555-1111         Gold  2023-01-10  1998-04-12  \n",
       "1  555-2222       Silver  2023-03-15  1995-09-22  \n",
       "2  555-3333       Bronze  2024-01-02  2000-07-18  \n",
       "3  555-4444         Gold  2022-11-08  1992-02-05  \n",
       "4  555-5555       Silver  2023-06-21  1999-11-30  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customers_pdf = get_sql_dataframe(\n",
    "    \"SELECT * FROM customers\",\n",
    "    **mysql_args\n",
    ")\n",
    "\n",
    "print(customers_pdf.shape)\n",
    "customers_pdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa90283f",
   "metadata": {},
   "source": [
    "customer dim - MySQL source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "860e8f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+----------+---------+--------------------+--------+------------+-----------+----------+--------------------------+\n",
      "|customer_id|customer_code|first_name|last_name|email               |phone   |loyalty_tier|signup_date|birthdate |created_at                |\n",
      "+-----------+-------------+----------+---------+--------------------+--------+------------+-----------+----------+--------------------------+\n",
      "|1          |CUST-001     |Emma      |Johnson  |emma.j@example.com  |555-1111|Gold        |2023-01-10 |1998-04-12|2025-12-14 09:08:29.984037|\n",
      "|2          |CUST-002     |Liam      |Smith    |liam.s@example.com  |555-2222|Silver      |2023-03-15 |1995-09-22|2025-12-14 09:08:29.984037|\n",
      "|3          |CUST-003     |Olivia    |Brown    |olivia.b@example.com|555-3333|Bronze      |2024-01-02 |2000-07-18|2025-12-14 09:08:29.984037|\n",
      "|4          |CUST-004     |Noah      |Davis    |noah.d@example.com  |555-4444|Gold        |2022-11-08 |1992-02-05|2025-12-14 09:08:29.984037|\n",
      "|5          |CUST-005     |Ava       |Wilson   |ava.w@example.com   |555-5555|Silver      |2023-06-21 |1999-11-30|2025-12-14 09:08:29.984037|\n",
      "+-----------+-------------+----------+---------+--------------------+--------+------------+-----------+----------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customers_sdf = (\n",
    "    spark.createDataFrame(customers_pdf)\n",
    "         .withColumn(\"customer_id\", F.col(\"customer_id\").cast(\"int\"))\n",
    "         .withColumn(\"signup_date\", F.to_date(\"signup_date\"))\n",
    "         .withColumn(\"birthdate\", F.to_date(\"birthdate\"))\n",
    "         .withColumn(\"created_at\", F.current_timestamp())\n",
    ")\n",
    "\n",
    "customers_sdf.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b559fdb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+----------+---------+--------------------+--------+------------+-----------+----------+-------------------------+\n",
      "|customer_id|customer_code|first_name|last_name|email               |phone   |loyalty_tier|signup_date|birthdate |created_at               |\n",
      "+-----------+-------------+----------+---------+--------------------+--------+------------+-----------+----------+-------------------------+\n",
      "|3          |CUST-003     |Olivia    |Brown    |olivia.b@example.com|555-3333|Bronze      |2024-01-02 |2000-07-18|2025-12-14 09:08:56.76229|\n",
      "|2          |CUST-002     |Liam      |Smith    |liam.s@example.com  |555-2222|Silver      |2023-03-15 |1995-09-22|2025-12-14 09:08:56.76229|\n",
      "|1          |CUST-001     |Emma      |Johnson  |emma.j@example.com  |555-1111|Gold        |2023-01-10 |1998-04-12|2025-12-14 09:08:56.76229|\n",
      "|5          |CUST-005     |Ava       |Wilson   |ava.w@example.com   |555-5555|Silver      |2023-06-21 |1999-11-30|2025-12-14 09:08:56.76229|\n",
      "|4          |CUST-004     |Noah      |Davis    |noah.d@example.com  |555-4444|Gold        |2022-11-08 |1992-02-05|2025-12-14 09:08:56.76229|\n",
      "+-----------+-------------+----------+---------+--------------------+--------+------------+-----------+----------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_customer_path = str(WAREHOUSE_DIR / \"dim_customer\")\n",
    "\n",
    "(customers_sdf\n",
    " .write.format(\"delta\")\n",
    " .mode(\"overwrite\")\n",
    " .save(dim_customer_path)\n",
    ")\n",
    "\n",
    "spark.read.format(\"delta\").load(dim_customer_path).show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a62295b",
   "metadata": {},
   "source": [
    "menu item dim - mongoDB noSQL source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f787d0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+----------+--------+-------------+---------+---------+----------------+------------+------------+\n",
      "|_id                     |base_price|category|dietary_flags|is_active|item_code|item_name       |menu_item_id|sub_category|\n",
      "+------------------------+----------+--------+-------------+---------+---------+----------------+------------+------------+\n",
      "|693ecb5dc3b15cd27611d255|13.99     |Entree  |             |true     |ITEM-001 |Cheeseburger    |1           |Burgers     |\n",
      "|693ecb5dc3b15cd27611d256|14.49     |Entree  |V            |true     |ITEM-002 |Veggie Burger   |2           |Burgers     |\n",
      "|693ecb5dc3b15cd27611d257|12.5      |Entree  |             |true     |ITEM-003 |Caesar Salad    |3           |Salads      |\n",
      "|693ecb5dc3b15cd27611d258|4.5       |Side    |V            |true     |ITEM-004 |Fries           |4           |Potatoes    |\n",
      "|693ecb5dc3b15cd27611d259|15.0      |Entree  |V            |true     |ITEM-005 |Margherita Pizza|5           |Pizza       |\n",
      "+------------------------+----------+--------+-------------+---------+---------+----------------+------------+------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "docs = list(menu_coll.find({}))\n",
    "\n",
    "for d in docs:\n",
    "    d[\"_id\"] = str(d[\"_id\"])\n",
    "\n",
    "menu_sdf = spark.createDataFrame(docs)\n",
    "menu_sdf.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9361ecd6",
   "metadata": {},
   "source": [
    "menu item dim transformation and delta storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8b5615f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+----------------+--------+-------------+----------+-------------+---------+--------------------------+\n",
      "|menu_item_id|item_code|item_name       |category|sub_category |base_price|dietary_flags|is_active|created_at                |\n",
      "+------------+---------+----------------+--------+-------------+----------+-------------+---------+--------------------------+\n",
      "|5           |ITEM-005 |Margherita Pizza|Entree  |Pizza        |15.0      |V            |true     |2025-12-14 09:38:53.492071|\n",
      "|7           |ITEM-007 |Iced Tea        |Drink   |Non-alcoholic|3.25      |V            |true     |2025-12-14 09:38:53.492071|\n",
      "|2           |ITEM-002 |Veggie Burger   |Entree  |Burgers      |14.49     |V            |true     |2025-12-14 09:38:53.492071|\n",
      "|9           |ITEM-009 |Chocolate Cake  |Dessert |Cake         |7.25      |V            |true     |2025-12-14 09:38:53.492071|\n",
      "|6           |ITEM-006 |Pepperoni Pizza |Entree  |Pizza        |16.5      |             |true     |2025-12-14 09:38:53.492071|\n",
      "|1           |ITEM-001 |Cheeseburger    |Entree  |Burgers      |13.99     |             |true     |2025-12-14 09:38:53.492071|\n",
      "|10          |ITEM-010 |Salmon Bowl     |Entree  |Bowls        |19.5      |GF           |true     |2025-12-14 09:38:53.492071|\n",
      "|3           |ITEM-003 |Caesar Salad    |Entree  |Salads       |12.5      |             |true     |2025-12-14 09:38:53.492071|\n",
      "|4           |ITEM-004 |Fries           |Side    |Potatoes     |4.5       |V            |true     |2025-12-14 09:38:53.492071|\n",
      "|8           |ITEM-008 |Latte           |Drink   |Coffee       |4.75      |V            |true     |2025-12-14 09:38:53.492071|\n",
      "+------------+---------+----------------+--------+-------------+----------+-------------+---------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "menu_sdf = (\n",
    "    menu_sdf\n",
    "      .withColumn(\"menu_item_id\", F.col(\"menu_item_id\").cast(\"int\"))\n",
    "      .withColumn(\"base_price\", F.col(\"base_price\").cast(\"double\"))\n",
    "      .withColumn(\"is_active\", F.col(\"is_active\").cast(\"boolean\"))\n",
    "      .withColumn(\"created_at\", F.current_timestamp())\n",
    "      .select(\n",
    "          \"menu_item_id\",\n",
    "          \"item_code\",\n",
    "          \"item_name\",\n",
    "          \"category\",\n",
    "          \"sub_category\",\n",
    "          \"base_price\",\n",
    "          \"dietary_flags\",\n",
    "          \"is_active\",\n",
    "          \"created_at\"\n",
    "      )\n",
    ")\n",
    "\n",
    "dim_menu_item_path = (WAREHOUSE_DIR / \"dim_menu_item\").resolve().as_posix()\n",
    "\n",
    "menu_sdf.write.format(\"delta\").mode(\"overwrite\").save(dim_menu_item_path)\n",
    "\n",
    "spark.read.format(\"delta\").load(dim_menu_item_path).show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ad5232",
   "metadata": {},
   "source": [
    "lakehouse dir struct and config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "78132834",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = os.path.join(os.getcwd(), 'data')\n",
    "orders_stream_dir = os.path.join(os.getcwd(), \"data\", \"orders_stream\")\n",
    "\n",
    "\n",
    "dest_database = \"restaurant_dlh\"\n",
    "sql_warehouse_dir = os.path.abspath('spark-warehouse')       \n",
    "database_dir = os.path.join(sql_warehouse_dir, dest_database)\n",
    "\n",
    "orders_output_bronze = os.path.join(database_dir, 'orders_bronze')\n",
    "orders_output_silver = os.path.join(database_dir, 'orders_silver')\n",
    "orders_output_gold   = os.path.join(database_dir, 'orders_gold')\n",
    "\n",
    "orders_checkpoint_bronze = os.path.join(orders_output_bronze, '_checkpoint')\n",
    "orders_checkpoint_silver = os.path.join(orders_output_silver, '_checkpoint')\n",
    "orders_checkpoint_gold   = os.path.join(orders_output_gold, '_checkpoint')\n",
    "\n",
    "os.makedirs(orders_stream_dir, exist_ok=True)\n",
    "os.makedirs(database_dir, exist_ok=True)\n",
    "\n",
    "def remove_directory_tree(path: str):\n",
    "    try:\n",
    "        if os.path.exists(path):\n",
    "            shutil.rmtree(path)\n",
    "            return f\"Directory '{path}' removed.\"\n",
    "        return f\"Directory '{path}' does not exist.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bf234bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, random, datetime as dt, os\n",
    "\n",
    "dim_customer_path   = (WAREHOUSE_DIR / \"dim_customer\").resolve().as_posix()\n",
    "dim_employee_path   = (WAREHOUSE_DIR / \"dim_employee\").resolve().as_posix()\n",
    "dim_order_type_path = (WAREHOUSE_DIR / \"dim_order_type\").resolve().as_posix()\n",
    "dim_menu_item_path  = (WAREHOUSE_DIR / \"dim_menu_item\").resolve().as_posix()\n",
    "\n",
    "customers   = [r[\"customer_id\"] for r in spark.read.format(\"delta\").load(dim_customer_path).select(\"customer_id\").limit(500).collect()]\n",
    "employees   = [r[\"employee_id\"] for r in spark.read.format(\"delta\").load(dim_employee_path).select(\"employee_id\").collect()]\n",
    "order_types = [r[\"order_type_id\"] for r in spark.read.format(\"delta\").load(dim_order_type_path).select(\"order_type_id\").collect()]\n",
    "menu_items  = [r[\"menu_item_id\"] for r in spark.read.format(\"delta\").load(dim_menu_item_path).select(\"menu_item_id\").collect()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452124cb",
   "metadata": {},
   "source": [
    "simulated streaming order event gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "45c5acf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_order_drop(drop_idx: int, n_orders: int = 15):\n",
    "    now = dt.datetime.now()\n",
    "    out_path = os.path.join(orders_stream_dir, f\"orders_drop_{drop_idx:02d}.json\")\n",
    "\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for i in range(n_orders):\n",
    "            order_id = drop_idx * 100000 + i + 1\n",
    "\n",
    "            lines = []\n",
    "            for _ in range(random.randint(1, 5)):\n",
    "                lines.append({\n",
    "                    \"menu_item_id\": int(random.choice(menu_items)),\n",
    "                    \"quantity\": int(random.randint(1, 3))\n",
    "                })\n",
    "\n",
    "            order = {\n",
    "                \"order_id\": int(order_id),\n",
    "                \"customer_id\": int(random.choice(customers)),\n",
    "                \"employee_id\": int(random.choice(employees)),\n",
    "                \"order_type_id\": int(random.choice(order_types)),\n",
    "                \"order_ts\": (now - dt.timedelta(minutes=random.randint(0, 240))).isoformat(),\n",
    "                \"lines\": lines\n",
    "            }\n",
    "\n",
    "            f.write(json.dumps(order) + \"\\n\")\n",
    "\n",
    "    return out_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30075871",
   "metadata": {},
   "source": [
    "simulate order drops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6a609949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\evasb_eqbyhhj\\Documents\\DS-2002-Capstone\\notebooks\\data\\orders_stream\\orders_drop_01.json\n",
      "c:\\Users\\evasb_eqbyhhj\\Documents\\DS-2002-Capstone\\notebooks\\data\\orders_stream\\orders_drop_02.json\n",
      "c:\\Users\\evasb_eqbyhhj\\Documents\\DS-2002-Capstone\\notebooks\\data\\orders_stream\\orders_drop_03.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['orders_drop_01.json', 'orders_drop_02.json', 'orders_drop_03.json']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(write_order_drop(1, 15))\n",
    "print(write_order_drop(2, 15))\n",
    "print(write_order_drop(3, 15))\n",
    "\n",
    "os.listdir(orders_stream_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7dc0b6",
   "metadata": {},
   "source": [
    "reg dim tables for SQL queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9e0e4619",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.format(\"delta\").load((WAREHOUSE_DIR/\"dim_customer\").resolve().as_posix()).createOrReplaceTempView(\"dim_customer\")\n",
    "spark.read.format(\"delta\").load((WAREHOUSE_DIR/\"dim_employee\").resolve().as_posix()).createOrReplaceTempView(\"dim_employee\")\n",
    "spark.read.format(\"delta\").load((WAREHOUSE_DIR/\"dim_order_type\").resolve().as_posix()).createOrReplaceTempView(\"dim_order_type\")\n",
    "spark.read.format(\"delta\").load((WAREHOUSE_DIR/\"dim_menu_item\").resolve().as_posix()).createOrReplaceTempView(\"dim_menu_item\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6366c0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: long (nullable = true)\n",
      " |-- employee_id: long (nullable = true)\n",
      " |-- lines: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- menu_item_id: long (nullable = true)\n",
      " |    |    |-- quantity: long (nullable = true)\n",
      " |-- order_id: long (nullable = true)\n",
      " |-- order_ts: string (nullable = true)\n",
      " |-- order_type_id: long (nullable = true)\n",
      "\n",
      "+-----------+-----------+------------------------------------------+--------+--------------------------+-------------+\n",
      "|customer_id|employee_id|lines                                     |order_id|order_ts                  |order_type_id|\n",
      "+-----------+-----------+------------------------------------------+--------+--------------------------+-------------+\n",
      "|4          |6          |[{9, 1}, {8, 1}, {10, 2}, {10, 3}, {5, 1}]|100001  |2025-12-14T09:26:35.004248|1            |\n",
      "|5          |13         |[{3, 1}, {6, 1}, {6, 1}, {9, 1}, {6, 3}]  |100002  |2025-12-14T09:57:35.004248|2            |\n",
      "|1          |3          |[{8, 1}, {7, 1}]                          |100003  |2025-12-14T07:58:35.004248|2            |\n",
      "+-----------+-----------+------------------------------------------+--------+--------------------------+-------------+\n",
      "only showing top 3 rows\n"
     ]
    }
   ],
   "source": [
    "sample_file = os.path.join(orders_stream_dir, os.listdir(orders_stream_dir)[0])\n",
    "\n",
    "df_one = spark.read.json(sample_file)\n",
    "orders_schema = df_one.schema\n",
    "\n",
    "df_one.printSchema()\n",
    "df_one.show(3, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "16c071a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: long (nullable = true)\n",
      " |-- employee_id: long (nullable = true)\n",
      " |-- lines: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- menu_item_id: long (nullable = true)\n",
      " |    |    |-- quantity: long (nullable = true)\n",
      " |-- order_id: long (nullable = true)\n",
      " |-- order_ts: string (nullable = true)\n",
      " |-- order_type_id: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_orders_bronze = (\n",
    "    spark.readStream\n",
    "        .schema(orders_schema)\n",
    "        .option(\"maxFilesPerTrigger\", 1)\n",
    "        .json(orders_stream_dir)\n",
    ")\n",
    "\n",
    "df_orders_bronze.isStreaming\n",
    "df_orders_bronze.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf36ddf",
   "metadata": {},
   "source": [
    "BRONZE LAYER - streaming order ing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a3ea2f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_bronze_query = (\n",
    "    df_orders_bronze\n",
    "      .writeStream\n",
    "      .format(\"delta\")\n",
    "      .outputMode(\"append\")\n",
    "      .queryName(\"orders_bronze\")\n",
    "      .option(\"checkpointLocation\", orders_checkpoint_bronze)\n",
    "      .trigger(availableNow=True)\n",
    "      .start(orders_output_bronze)\n",
    ")\n",
    "\n",
    "orders_bronze_query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "676ca275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orders_output_bronze exists: True\n",
      "_delta_log exists: True\n",
      "+-----------+-----------+------------------------------------------+--------+--------------------------+-------------+\n",
      "|customer_id|employee_id|lines                                     |order_id|order_ts                  |order_type_id|\n",
      "+-----------+-----------+------------------------------------------+--------+--------------------------+-------------+\n",
      "|4          |6          |[{9, 1}, {8, 1}, {10, 2}, {10, 3}, {5, 1}]|100001  |2025-12-14T09:26:35.004248|1            |\n",
      "|5          |13         |[{3, 1}, {6, 1}, {6, 1}, {9, 1}, {6, 3}]  |100002  |2025-12-14T09:57:35.004248|2            |\n",
      "|1          |3          |[{8, 1}, {7, 1}]                          |100003  |2025-12-14T07:58:35.004248|2            |\n",
      "|5          |20         |[{1, 1}, {1, 1}, {3, 2}]                  |100004  |2025-12-14T09:02:35.004248|2            |\n",
      "|3          |17         |[{10, 2}, {6, 1}]                         |100005  |2025-12-14T07:12:35.004248|1            |\n",
      "+-----------+-----------+------------------------------------------+--------+--------------------------+-------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(\"orders_output_bronze exists:\", os.path.exists(orders_output_bronze))\n",
    "print(\"_delta_log exists:\", os.path.exists(os.path.join(orders_output_bronze, \"_delta_log\")))\n",
    "\n",
    "spark.read.format(\"delta\").load(orders_output_bronze).show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d2f53695",
   "metadata": {},
   "outputs": [],
   "source": [
    "(spark.readStream\n",
    "    .format(\"delta\")\n",
    "    .load(orders_output_bronze)\n",
    "    .createOrReplaceTempView(\"orders_bronze_stream\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4475f27",
   "metadata": {},
   "source": [
    "SILVER LAYER - Fact Table Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "298c1c23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_silver_temp = \"\"\"\n",
    "CREATE OR REPLACE TEMP VIEW fact_order_line_temp AS\n",
    "SELECT\n",
    "    x.order_id,\n",
    "    x.customer_id,\n",
    "    x.employee_id,\n",
    "    x.order_type_id,\n",
    "    x.order_ts,\n",
    "    x.date_id,\n",
    "\n",
    "    x.menu_item_id,\n",
    "    x.quantity,\n",
    "\n",
    "    current_timestamp() AS processed_time,\n",
    "\n",
    "    -- joined dim attributes (multiple joins = rubric win)\n",
    "    c.first_name AS customer_first_name,\n",
    "    c.last_name  AS customer_last_name,\n",
    "    e.role       AS employee_role,\n",
    "    ot.order_type_name,\n",
    "    mi.item_name,\n",
    "    mi.category,\n",
    "    mi.sub_category,\n",
    "    mi.base_price\n",
    "\n",
    "FROM (\n",
    "    SELECT\n",
    "        b.order_id,\n",
    "        b.customer_id,\n",
    "        b.employee_id,\n",
    "        b.order_type_id,\n",
    "        to_timestamp(b.order_ts) AS order_ts,\n",
    "        CAST(date_format(to_timestamp(b.order_ts), 'yyyyMMdd') AS INT) AS date_id,\n",
    "        line.menu_item_id AS menu_item_id,\n",
    "        CAST(line.quantity AS INT) AS quantity\n",
    "    FROM orders_bronze_stream b\n",
    "    LATERAL VIEW explode(b.lines) lv AS line\n",
    ") x\n",
    "JOIN dim_customer   c  ON x.customer_id = c.customer_id\n",
    "JOIN dim_employee   e  ON x.employee_id = e.employee_id\n",
    "JOIN dim_order_type ot ON x.order_type_id = ot.order_type_id\n",
    "JOIN dim_menu_item  mi ON x.menu_item_id = mi.menu_item_id\n",
    "WHERE x.order_id IS NOT NULL\n",
    "\"\"\"\n",
    "spark.sql(sql_silver_temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4fbf3e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_silver_query = (\n",
    "    spark.table(\"fact_order_line_temp\")\n",
    "      .writeStream\n",
    "      .format(\"delta\")\n",
    "      .outputMode(\"append\")\n",
    "      .queryName(\"orders_silver_fact_order_line\")\n",
    "      .option(\"checkpointLocation\", orders_checkpoint_silver)\n",
    "      .trigger(availableNow=True)\n",
    "      .start(orders_output_silver)\n",
    ")\n",
    "\n",
    "orders_silver_query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8ace4866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orders_output_silver exists: True\n",
      "_delta_log exists: True\n",
      "+--------+-----------+-----------+-------------+--------------------------+--------+------------+--------+-----------------------+-------------------+------------------+-------------+---------------+----------------+--------+------------+----------+\n",
      "|order_id|customer_id|employee_id|order_type_id|order_ts                  |date_id |menu_item_id|quantity|processed_time         |customer_first_name|customer_last_name|employee_role|order_type_name|item_name       |category|sub_category|base_price|\n",
      "+--------+-----------+-----------+-------------+--------------------------+--------+------------+--------+-----------------------+-------------------+------------------+-------------+---------------+----------------+--------+------------+----------+\n",
      "|200001  |2          |3          |3            |2025-12-14 07:27:35.006244|20251214|1           |2       |2025-12-14 10:31:44.897|Liam               |Smith             |Bartender    |Delivery       |Cheeseburger    |Entree  |Burgers     |13.99     |\n",
      "|200001  |2          |3          |3            |2025-12-14 07:27:35.006244|20251214|5           |3       |2025-12-14 10:31:44.897|Liam               |Smith             |Bartender    |Delivery       |Margherita Pizza|Entree  |Pizza       |15.0      |\n",
      "|200002  |1          |19         |1            |2025-12-14 06:43:35.006244|20251214|1           |2       |2025-12-14 10:31:44.897|Emma               |Johnson           |Server       |Dine-in        |Cheeseburger    |Entree  |Burgers     |13.99     |\n",
      "|200002  |1          |19         |1            |2025-12-14 06:43:35.006244|20251214|6           |2       |2025-12-14 10:31:44.897|Emma               |Johnson           |Server       |Dine-in        |Pepperoni Pizza |Entree  |Pizza       |16.5      |\n",
      "|200002  |1          |19         |1            |2025-12-14 06:43:35.006244|20251214|5           |1       |2025-12-14 10:31:44.897|Emma               |Johnson           |Server       |Dine-in        |Margherita Pizza|Entree  |Pizza       |15.0      |\n",
      "|200003  |1          |3          |2            |2025-12-14 08:54:35.006244|20251214|6           |2       |2025-12-14 10:31:44.897|Emma               |Johnson           |Bartender    |Takeout        |Pepperoni Pizza |Entree  |Pizza       |16.5      |\n",
      "|200003  |1          |3          |2            |2025-12-14 08:54:35.006244|20251214|10          |3       |2025-12-14 10:31:44.897|Emma               |Johnson           |Bartender    |Takeout        |Salmon Bowl     |Entree  |Bowls       |19.5      |\n",
      "|200003  |1          |3          |2            |2025-12-14 08:54:35.006244|20251214|3           |2       |2025-12-14 10:31:44.897|Emma               |Johnson           |Bartender    |Takeout        |Caesar Salad    |Entree  |Salads      |12.5      |\n",
      "|200003  |1          |3          |2            |2025-12-14 08:54:35.006244|20251214|2           |3       |2025-12-14 10:31:44.897|Emma               |Johnson           |Bartender    |Takeout        |Veggie Burger   |Entree  |Burgers     |14.49     |\n",
      "|200003  |1          |3          |2            |2025-12-14 08:54:35.006244|20251214|9           |2       |2025-12-14 10:31:44.897|Emma               |Johnson           |Bartender    |Takeout        |Chocolate Cake  |Dessert |Cake        |7.25      |\n",
      "+--------+-----------+-----------+-------------+--------------------------+--------+------------+--------+-----------------------+-------------------+------------------+-------------+---------------+----------------+--------+------------+----------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(\"orders_output_silver exists:\", os.path.exists(orders_output_silver))\n",
    "print(\"_delta_log exists:\", os.path.exists(os.path.join(orders_output_silver, \"_delta_log\")))\n",
    "\n",
    "spark.read.format(\"delta\").load(orders_output_silver).show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "497a0395",
   "metadata": {},
   "outputs": [],
   "source": [
    "(spark.readStream\n",
    "    .format(\"delta\")\n",
    "    .load(orders_output_silver)\n",
    "    .createOrReplaceTempView(\"orders_silver_stream\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6992514c",
   "metadata": {},
   "source": [
    "Gold Layer - agg sales metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cf5842f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_gold_temp = \"\"\"\n",
    "CREATE OR REPLACE TEMP VIEW gold_daily_item_sales_temp AS\n",
    "SELECT\n",
    "    date_id,\n",
    "    category,\n",
    "    item_name,\n",
    "    SUM(quantity) AS units_sold,\n",
    "    ROUND(SUM(quantity * base_price), 2) AS gross_sales\n",
    "FROM orders_silver_stream\n",
    "GROUP BY date_id, category, item_name\n",
    "\"\"\"\n",
    "spark.sql(sql_gold_temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "69b0a265",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_gold_query = (\n",
    "    spark.table(\"gold_daily_item_sales_temp\")\n",
    "      .writeStream\n",
    "      .format(\"delta\")\n",
    "      .outputMode(\"complete\")\n",
    "      .queryName(\"orders_gold_daily_item_sales\")\n",
    "      .option(\"checkpointLocation\", orders_checkpoint_gold)\n",
    "      .trigger(availableNow=True)\n",
    "      .start(orders_output_gold)\n",
    ")\n",
    "\n",
    "orders_gold_query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7cc52d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+----------------+----------+-----------+\n",
      "|date_id |category|item_name       |units_sold|gross_sales|\n",
      "+--------+--------+----------------+----------+-----------+\n",
      "|20251214|Entree  |Salmon Bowl     |32        |624.0      |\n",
      "|20251214|Entree  |Cheeseburger    |43        |601.57     |\n",
      "|20251214|Entree  |Pepperoni Pizza |32        |528.0      |\n",
      "|20251214|Entree  |Veggie Burger   |33        |478.17     |\n",
      "|20251214|Entree  |Margherita Pizza|31        |465.0      |\n",
      "|20251214|Entree  |Caesar Salad    |31        |387.5      |\n",
      "|20251214|Dessert |Chocolate Cake  |13        |94.25      |\n",
      "|20251214|Side    |Fries           |16        |72.0       |\n",
      "|20251214|Drink   |Latte           |13        |61.75      |\n",
      "|20251214|Drink   |Iced Tea        |9         |29.25      |\n",
      "+--------+--------+----------------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"delta\").load(orders_output_gold) \\\n",
    "     .orderBy(\"gross_sales\", ascending=False) \\\n",
    "     .show(20, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ae8ca8",
   "metadata": {},
   "source": [
    "First query - analytics top customers by total spend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0f094374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+------------------+-----------+-----------+\n",
      "|customer_id|customer_first_name|customer_last_name|total_spend|total_items|\n",
      "+-----------+-------------------+------------------+-----------+-----------+\n",
      "|1          |Emma               |Johnson           |825.24     |65         |\n",
      "|4          |Noah               |Davis             |728.36     |56         |\n",
      "|2          |Liam               |Smith             |681.87     |47         |\n",
      "|5          |Ava                |Wilson            |582.64     |45         |\n",
      "|3          |Olivia             |Brown             |523.38     |40         |\n",
      "+-----------+-------------------+------------------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_top_customers = f\"\"\"\n",
    "SELECT\n",
    "    customer_id,\n",
    "    customer_first_name,\n",
    "    customer_last_name,\n",
    "    ROUND(SUM(quantity * base_price), 2) AS total_spend,\n",
    "    SUM(quantity) AS total_items\n",
    "FROM delta.`{orders_output_silver}`\n",
    "GROUP BY customer_id, customer_first_name, customer_last_name\n",
    "ORDER BY total_spend DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "spark.sql(sql_top_customers).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6717f94",
   "metadata": {},
   "source": [
    "Second analytics query - sales by order type and category over time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3f4d8e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+--------+----------+-----------+\n",
      "|date_id |order_type_name|category|units_sold|gross_sales|\n",
      "+--------+---------------+--------+----------+-----------+\n",
      "|20251214|Delivery       |Entree  |94        |1412.06    |\n",
      "|20251214|Takeout        |Entree  |66        |1022.85    |\n",
      "|20251214|Dine-in        |Entree  |42        |649.33     |\n",
      "|20251214|Delivery       |Dessert |6         |43.5       |\n",
      "|20251214|Takeout        |Drink   |9         |41.25      |\n",
      "|20251214|Takeout        |Dessert |5         |36.25      |\n",
      "|20251214|Delivery       |Side    |7         |31.5       |\n",
      "|20251214|Delivery       |Drink   |8         |27.5       |\n",
      "|20251214|Dine-in        |Side    |5         |22.5       |\n",
      "|20251214|Dine-in        |Drink   |5         |22.25      |\n",
      "|20251214|Takeout        |Side    |4         |18.0       |\n",
      "|20251214|Dine-in        |Dessert |2         |14.5       |\n",
      "+--------+---------------+--------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_order_type_category = f\"\"\"\n",
    "SELECT\n",
    "    date_id,\n",
    "    order_type_name,\n",
    "    category,\n",
    "    SUM(quantity) AS units_sold,\n",
    "    ROUND(SUM(quantity * base_price), 2) AS gross_sales\n",
    "FROM delta.`{orders_output_silver}`\n",
    "GROUP BY date_id, order_type_name, category\n",
    "ORDER BY date_id DESC, gross_sales DESC\n",
    "\"\"\"\n",
    "spark.sql(sql_order_type_category).show(50, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6927ae60",
   "metadata": {},
   "source": [
    "third analytics query - employee sales performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fd630f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------+--------------+----------+\n",
      "|employee_id|employee_role|revenue|orders_handled|items_sold|\n",
      "+-----------+-------------+-------+--------------+----------+\n",
      "|3          |Bartender    |459.44 |6             |37        |\n",
      "|17         |Bartender    |391.1  |5             |28        |\n",
      "|4          |Manager      |333.89 |5             |29        |\n",
      "|19         |Server       |318.98 |3             |22        |\n",
      "|8          |Server       |254.94 |4             |19        |\n",
      "|12         |Dishwasher   |219.72 |4             |21        |\n",
      "|6          |Server       |218.96 |3             |15        |\n",
      "|5          |Host         |203.67 |2             |14        |\n",
      "|14         |Server       |194.7  |2             |19        |\n",
      "|18         |Server       |164.95 |1             |11        |\n",
      "|20         |Host         |139.46 |3             |9         |\n",
      "|9          |Bartender    |131.98 |1             |9         |\n",
      "|13         |Host         |102.25 |1             |7         |\n",
      "|10         |Kitchen      |73.47  |1             |5         |\n",
      "|7          |Server       |69.0   |1             |4         |\n",
      "|16         |Kitchen      |36.0   |2             |2         |\n",
      "|2          |Server       |28.98  |1             |2         |\n",
      "+-----------+-------------+-------+--------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_employee_perf = f\"\"\"\n",
    "SELECT\n",
    "    employee_id,\n",
    "    employee_role,\n",
    "    ROUND(SUM(quantity * base_price), 2) AS revenue,\n",
    "    COUNT(DISTINCT order_id) AS orders_handled,\n",
    "    SUM(quantity) AS items_sold\n",
    "FROM delta.`{orders_output_silver}`\n",
    "GROUP BY employee_id, employee_role\n",
    "ORDER BY revenue DESC\n",
    "\"\"\"\n",
    "spark.sql(sql_employee_perf).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc483b7",
   "metadata": {},
   "source": [
    "Dim date (this was generated in mySQL using the script from class and this is j extracting it here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "62043951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1096, 23)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_key</th>\n",
       "      <th>full_date</th>\n",
       "      <th>date_name</th>\n",
       "      <th>date_name_us</th>\n",
       "      <th>date_name_eu</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>day_name_of_week</th>\n",
       "      <th>day_of_month</th>\n",
       "      <th>day_of_year</th>\n",
       "      <th>weekday_weekend</th>\n",
       "      <th>...</th>\n",
       "      <th>is_last_day_of_month</th>\n",
       "      <th>calendar_quarter</th>\n",
       "      <th>calendar_year</th>\n",
       "      <th>calendar_year_month</th>\n",
       "      <th>calendar_year_qtr</th>\n",
       "      <th>fiscal_month_of_year</th>\n",
       "      <th>fiscal_quarter</th>\n",
       "      <th>fiscal_year</th>\n",
       "      <th>fiscal_year_month</th>\n",
       "      <th>fiscal_year_qtr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20240101</td>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>2024/01/01</td>\n",
       "      <td>01/01/2024</td>\n",
       "      <td>01/01/2024</td>\n",
       "      <td>2</td>\n",
       "      <td>Monday</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Weekday</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>2024</td>\n",
       "      <td>2024-01</td>\n",
       "      <td>2024Q1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>2024</td>\n",
       "      <td>2024-07</td>\n",
       "      <td>2024Q3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20240102</td>\n",
       "      <td>2024-01-02</td>\n",
       "      <td>2024/01/02</td>\n",
       "      <td>01/02/2024</td>\n",
       "      <td>02/01/2024</td>\n",
       "      <td>3</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Weekday</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>2024</td>\n",
       "      <td>2024-01</td>\n",
       "      <td>2024Q1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>2024</td>\n",
       "      <td>2024-07</td>\n",
       "      <td>2024Q3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20240103</td>\n",
       "      <td>2024-01-03</td>\n",
       "      <td>2024/01/03</td>\n",
       "      <td>01/03/2024</td>\n",
       "      <td>03/01/2024</td>\n",
       "      <td>4</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Weekday</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>2024</td>\n",
       "      <td>2024-01</td>\n",
       "      <td>2024Q1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>2024</td>\n",
       "      <td>2024-07</td>\n",
       "      <td>2024Q3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20240104</td>\n",
       "      <td>2024-01-04</td>\n",
       "      <td>2024/01/04</td>\n",
       "      <td>01/04/2024</td>\n",
       "      <td>04/01/2024</td>\n",
       "      <td>5</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Weekday</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>2024</td>\n",
       "      <td>2024-01</td>\n",
       "      <td>2024Q1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>2024</td>\n",
       "      <td>2024-07</td>\n",
       "      <td>2024Q3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20240105</td>\n",
       "      <td>2024-01-05</td>\n",
       "      <td>2024/01/05</td>\n",
       "      <td>01/05/2024</td>\n",
       "      <td>05/01/2024</td>\n",
       "      <td>6</td>\n",
       "      <td>Friday</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>Weekday</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>2024</td>\n",
       "      <td>2024-01</td>\n",
       "      <td>2024Q1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>2024</td>\n",
       "      <td>2024-07</td>\n",
       "      <td>2024Q3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   date_key   full_date   date_name date_name_us date_name_eu  day_of_week  \\\n",
       "0  20240101  2024-01-01  2024/01/01   01/01/2024   01/01/2024            2   \n",
       "1  20240102  2024-01-02  2024/01/02   01/02/2024   02/01/2024            3   \n",
       "2  20240103  2024-01-03  2024/01/03   01/03/2024   03/01/2024            4   \n",
       "3  20240104  2024-01-04  2024/01/04   01/04/2024   04/01/2024            5   \n",
       "4  20240105  2024-01-05  2024/01/05   01/05/2024   05/01/2024            6   \n",
       "\n",
       "  day_name_of_week  day_of_month  day_of_year weekday_weekend  ...  \\\n",
       "0           Monday             1            1         Weekday  ...   \n",
       "1          Tuesday             2            2         Weekday  ...   \n",
       "2        Wednesday             3            3         Weekday  ...   \n",
       "3         Thursday             4            4         Weekday  ...   \n",
       "4           Friday             5            5         Weekday  ...   \n",
       "\n",
       "   is_last_day_of_month calendar_quarter  calendar_year calendar_year_month  \\\n",
       "0                     N                1           2024             2024-01   \n",
       "1                     N                1           2024             2024-01   \n",
       "2                     N                1           2024             2024-01   \n",
       "3                     N                1           2024             2024-01   \n",
       "4                     N                1           2024             2024-01   \n",
       "\n",
       "   calendar_year_qtr  fiscal_month_of_year fiscal_quarter fiscal_year  \\\n",
       "0             2024Q1                     7              3        2024   \n",
       "1             2024Q1                     7              3        2024   \n",
       "2             2024Q1                     7              3        2024   \n",
       "3             2024Q1                     7              3        2024   \n",
       "4             2024Q1                     7              3        2024   \n",
       "\n",
       "   fiscal_year_month  fiscal_year_qtr  \n",
       "0            2024-07           2024Q3  \n",
       "1            2024-07           2024Q3  \n",
       "2            2024-07           2024Q3  \n",
       "3            2024-07           2024Q3  \n",
       "4            2024-07           2024Q3  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_date_pdf = get_sql_dataframe(\"SELECT * FROM dim_date\", **mysql_args)\n",
    "print(dim_date_pdf.shape)\n",
    "dim_date_pdf.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "884934e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date_id: integer (nullable = true)\n",
      " |-- full_date: date (nullable = true)\n",
      " |-- date_name: string (nullable = true)\n",
      " |-- date_name_us: string (nullable = true)\n",
      " |-- date_name_eu: string (nullable = true)\n",
      " |-- day_of_week: long (nullable = true)\n",
      " |-- day_name_of_week: string (nullable = true)\n",
      " |-- day_of_month: long (nullable = true)\n",
      " |-- day_of_year: long (nullable = true)\n",
      " |-- weekday_weekend: string (nullable = true)\n",
      " |-- week_of_year: long (nullable = true)\n",
      " |-- month_name: string (nullable = true)\n",
      " |-- month_of_year: long (nullable = true)\n",
      " |-- is_last_day_of_month: string (nullable = true)\n",
      " |-- calendar_quarter: long (nullable = true)\n",
      " |-- calendar_year: long (nullable = true)\n",
      " |-- calendar_year_month: string (nullable = true)\n",
      " |-- calendar_year_qtr: string (nullable = true)\n",
      " |-- fiscal_month_of_year: long (nullable = true)\n",
      " |-- fiscal_quarter: long (nullable = true)\n",
      " |-- fiscal_year: long (nullable = true)\n",
      " |-- fiscal_year_month: string (nullable = true)\n",
      " |-- fiscal_year_qtr: string (nullable = true)\n",
      "\n",
      "+--------+----------+----------+------------+------------+-----------+----------------+------------+-----------+---------------+------------+----------+-------------+--------------------+----------------+-------------+-------------------+-----------------+--------------------+--------------+-----------+-----------------+---------------+\n",
      "|date_id |full_date |date_name |date_name_us|date_name_eu|day_of_week|day_name_of_week|day_of_month|day_of_year|weekday_weekend|week_of_year|month_name|month_of_year|is_last_day_of_month|calendar_quarter|calendar_year|calendar_year_month|calendar_year_qtr|fiscal_month_of_year|fiscal_quarter|fiscal_year|fiscal_year_month|fiscal_year_qtr|\n",
      "+--------+----------+----------+------------+------------+-----------+----------------+------------+-----------+---------------+------------+----------+-------------+--------------------+----------------+-------------+-------------------+-----------------+--------------------+--------------+-----------+-----------------+---------------+\n",
      "|20240101|2024-01-01|2024/01/01|01/01/2024  |01/01/2024  |2          |Monday          |1           |1          |Weekday        |1           |January   |1            |N                   |1               |2024         |2024-01            |2024Q1           |7                   |3             |2024       |2024-07          |2024Q3         |\n",
      "|20240102|2024-01-02|2024/01/02|01/02/2024  |02/01/2024  |3          |Tuesday         |2           |2          |Weekday        |1           |January   |1            |N                   |1               |2024         |2024-01            |2024Q1           |7                   |3             |2024       |2024-07          |2024Q3         |\n",
      "|20240103|2024-01-03|2024/01/03|01/03/2024  |03/01/2024  |4          |Wednesday       |3           |3          |Weekday        |1           |January   |1            |N                   |1               |2024         |2024-01            |2024Q1           |7                   |3             |2024       |2024-07          |2024Q3         |\n",
      "|20240104|2024-01-04|2024/01/04|01/04/2024  |04/01/2024  |5          |Thursday        |4           |4          |Weekday        |1           |January   |1            |N                   |1               |2024         |2024-01            |2024Q1           |7                   |3             |2024       |2024-07          |2024Q3         |\n",
      "|20240105|2024-01-05|2024/01/05|01/05/2024  |05/01/2024  |6          |Friday          |5           |5          |Weekday        |1           |January   |1            |N                   |1               |2024         |2024-01            |2024Q1           |7                   |3             |2024       |2024-07          |2024Q3         |\n",
      "+--------+----------+----------+------------+------------+-----------+----------------+------------+-----------+---------------+------------+----------+-------------+--------------------+----------------+-------------+-------------------+-----------------+--------------------+--------------+-----------+-----------------+---------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dim_date_sdf = (\n",
    "    spark.createDataFrame(dim_date_pdf)\n",
    "         .withColumn(\"date_key\", F.col(\"date_key\").cast(\"int\"))\n",
    "         .withColumnRenamed(\"date_key\", \"date_id\")\n",
    ")\n",
    "\n",
    "dim_date_sdf.printSchema()\n",
    "dim_date_sdf.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aaea524",
   "metadata": {},
   "source": [
    "adding to DL storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "22f7b965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote dim_date to: C:/Users/evasb_eqbyhhj/Documents/DS-2002-Capstone/notebooks/warehouse/dim_date\n"
     ]
    }
   ],
   "source": [
    "(dim_date_sdf\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .save(dim_date_path)\n",
    ")\n",
    "\n",
    "print(\"Wrote dim_date to:\", dim_date_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2224d005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+----------+------------+------------+-----------+----------------+------------+-----------+---------------+------------+----------+-------------+--------------------+----------------+-------------+-------------------+-----------------+--------------------+--------------+-----------+-----------------+---------------+\n",
      "|date_id |full_date |date_name |date_name_us|date_name_eu|day_of_week|day_name_of_week|day_of_month|day_of_year|weekday_weekend|week_of_year|month_name|month_of_year|is_last_day_of_month|calendar_quarter|calendar_year|calendar_year_month|calendar_year_qtr|fiscal_month_of_year|fiscal_quarter|fiscal_year|fiscal_year_month|fiscal_year_qtr|\n",
      "+--------+----------+----------+------------+------------+-----------+----------------+------------+-----------+---------------+------------+----------+-------------+--------------------+----------------+-------------+-------------------+-----------------+--------------------+--------------+-----------+-----------------+---------------+\n",
      "|20260514|2026-05-14|2026/05/14|05/14/2026  |14/05/2026  |5          |Thursday        |14          |134        |Weekday        |20          |May       |5            |N                   |2               |2026         |2026-05            |2026Q2           |11                  |4             |2026       |2026-11          |2026Q4         |\n",
      "|20260515|2026-05-15|2026/05/15|05/15/2026  |15/05/2026  |6          |Friday          |15          |135        |Weekday        |20          |May       |5            |N                   |2               |2026         |2026-05            |2026Q2           |11                  |4             |2026       |2026-11          |2026Q4         |\n",
      "|20260516|2026-05-16|2026/05/16|05/16/2026  |16/05/2026  |7          |Saturday        |16          |136        |Weekend        |20          |May       |5            |N                   |2               |2026         |2026-05            |2026Q2           |11                  |4             |2026       |2026-11          |2026Q4         |\n",
      "|20260517|2026-05-17|2026/05/17|05/17/2026  |17/05/2026  |1          |Sunday          |17          |137        |Weekend        |20          |May       |5            |N                   |2               |2026         |2026-05            |2026Q2           |11                  |4             |2026       |2026-11          |2026Q4         |\n",
      "|20260518|2026-05-18|2026/05/18|05/18/2026  |18/05/2026  |2          |Monday          |18          |138        |Weekday        |21          |May       |5            |N                   |2               |2026         |2026-05            |2026Q2           |11                  |4             |2026       |2026-11          |2026Q4         |\n",
      "+--------+----------+----------+------------+------------+-----------+----------------+------------+-----------+---------------+------------+----------+-------------+--------------------+----------------+-------------+-------------------+-----------------+--------------------+--------------+-----------+-----------------+---------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"delta\").load(dim_date_path).show(5, truncate=False)\n",
    "spark.read.format(\"delta\").load(dim_date_path).createOrReplaceTempView(\"dim_date\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25626cdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ds2002spark)",
   "language": "python",
   "name": "ds2002spark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
